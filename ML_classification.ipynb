{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# REMOVED\n",
    "#import torch\n",
    "#import transformers\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "from src.preprocess import *\n",
    "from src.models import *\n",
    "from src.lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e79b42",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "\n",
    "### kinds of cause:\n",
    "- 1 = No reason\n",
    "- 2 = Bias or abuse\n",
    "- 3 = Jobs and careers\n",
    "- 4 = Medication\n",
    "- 5 = Relationships\n",
    "- 6 = Alienation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1781aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Training and Testing files\n",
    "read_file_training = pd.read_csv('./data/added_CAMS_data.csv', encoding=\"ISO-8859-1\")\n",
    "read_file_testing = pd.read_csv('./data/IntentSDCNL_Testing.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "\n",
    "\n",
    "#training data\n",
    "df_train=pd.DataFrame(read_file_training,columns= ['selftext','cause', 'inference'])### \n",
    "df_train = df_train.rename(\n",
    "    columns={'selftext': 'text'})\n",
    "df_train= df_train.convert_dtypes()\n",
    "\n",
    "#testing data\n",
    "df_test=pd.DataFrame(read_file_testing,columns= ['selftext','ANNOTATIONS', 'Interpretations'])#### \n",
    "df_test = df_test.rename(columns={'selftext': 'text', 'ANNOTATIONS': 'cause', 'Interpretations': 'inference'})\n",
    "df_test= df_test.convert_dtypes()\n",
    "\n",
    "## Sampling file\n",
    "# df_samples=pd.read_csv('./data/sample.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "## add samples to training file\n",
    "\n",
    "frame = [df_train]\n",
    "df_train =  pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81b1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(df_test[df_test.text==\"emptypost\"].index, inplace=True)\n",
    "df_test.drop(df_test[df_test.cause==\" \"].index, inplace=True)\n",
    "df_test.drop(df_test[df_test.inference==\" \"].index, inplace=True)\n",
    "\n",
    "# print(df_train)\n",
    "\n",
    "df_train.drop(df_train[df_train.text==\"emptypost\"].index, inplace=True)\n",
    "df_train.drop(df_train[df_train.cause==\" \"].index, inplace=True)\n",
    "df_train.drop(df_train[df_train.inference==\" \"].index, inplace=True)\n",
    "\n",
    "df_test.dropna(inplace=True)\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a878dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"###### Class distribution of training data #####\")\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "df_train['cause'].value_counts().plot.pie(autopct = '%1.1f%%', labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731db500",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"###### Class distribution of testing data #####\")\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "df_test['cause'].value_counts().plot.pie(autopct = '%1.1f%%', labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca269e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['cause'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd664e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = [df_test, df_train]\n",
    "data =  pd.concat(frame)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b85644",
   "metadata": {},
   "source": [
    "#### A Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(df_train,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8bf9b",
   "metadata": {},
   "source": [
    "## Clean and preprocess the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f4d70",
   "metadata": {},
   "source": [
    "####  Clean Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2af759",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() \n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) \n",
    "    text = BAD_SYMBOLS_RE.sub('', text)     \n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "\n",
    "\n",
    "#training data\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "\n",
    "##testing data\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test['text'] = df_test['text'].apply(clean_text)\n",
    "df_test['text'] = df_test['text'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b991f54",
   "metadata": {},
   "source": [
    "#### A sample after cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46136ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(df_train,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a980db",
   "metadata": {},
   "source": [
    "#### Intialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5dd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some Global Variables\n",
    "max_nb_words = 60000\n",
    "max_features = 10543 # Maximum Number of words we want to include in our dictionary\n",
    "maxlen = 500 # No of words in question we want to create a sequence with\n",
    "emb_dim = 100 # Size of word to vec embedding we are usingtokenizer = Tokenizer(num_words=max_nb_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a645a",
   "metadata": {},
   "source": [
    "#### Initial Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354197dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_nb_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data['text'].values)# tokenizer on whole training and testing data\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(df_train['text'].values)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "print('Shape of data tensor:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd114a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (2877, 6)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies(df_train['cause']).values\n",
    "print('Shape of label tensor:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(df_test['text'].values)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "print('Shape of data tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = pd.get_dummies(df_test['cause']).values\n",
    "print('Shape of label tensor:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daadd61",
   "metadata": {},
   "source": [
    "### Finally some validation data from Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "749fdee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,random_state=0,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TrainingData',X_train.shape,Y_train.shape)\n",
    "print('TestingData',X_test.shape,Y_test.shape)\n",
    "print('ValidationData',X_val.shape,Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4464a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print a test sample\n",
    "#print('test example:', X_test[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8539e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(X_test[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8e9b0",
   "metadata": {},
   "source": [
    "#### Intialize Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = {}\n",
    "emb_dim = 100\n",
    "with open(f'./embeddings/glove.6B.{emb_dim}d.txt','r',encoding='utf-8') as file:\n",
    "\n",
    "    for row in file:\n",
    "        values = row.split(' ')\n",
    "        word = values[0]\n",
    "        weights = np.asarray([float(val) for val in values[1:]])\n",
    "        embedding_vectors[word] = weights\n",
    "print(f\"Size of vocabulary in GloVe: {len(embedding_vectors)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the embedding_matrix with zeros\n",
    "\n",
    "if max_nb_words is not None: \n",
    "    vocab_len = max_nb_words\n",
    "else:\n",
    "    vocab_len = len(word_index)+1\n",
    "embedding_matrix = np.zeros((vocab_len, emb_dim))\n",
    "oov_count = 0\n",
    "oov_words = []\n",
    "for word, idx in word_index.items():\n",
    "    if idx < vocab_len:\n",
    "        embedding_vector = embedding_vectors.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            oov_count += 1 \n",
    "            oov_words.append(word)\n",
    "#Check some of the out of vocabulary words\n",
    "print(f'A few out of valubulary words: {oov_words[0:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9286940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{oov_count} out of {vocab_len} words were OOV.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6a8c1",
   "metadata": {},
   "source": [
    "## NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ce9711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Global variables\n",
    "batch_size = 256\n",
    "epochs  = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330e923",
   "metadata": {},
   "source": [
    "#### a) LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_model(embedding_matrix,vocab_len,emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbf47c",
   "metadata": {},
   "source": [
    "##### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = lstm_model.fit(X_train, np.asarray(Y_train), validation_data=(X_val, np.asarray(Y_val)), batch_size = batch_size, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c327305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy\n",
    "plot_accuracy(history,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3daa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "train_test_accuracy(lstm_model,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5cfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report\n",
    "report_nn(lstm_model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba0f5c",
   "metadata": {},
   "source": [
    "#### b) CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model=CNN_model(embedding_matrix,vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a61c1",
   "metadata": {},
   "source": [
    "##### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2178e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = cnn_model.fit(X_train, np.asarray(Y_train), validation_data=(X_val, np.asarray(Y_val)),batch_size = batch_size, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy\n",
    "plot_accuracy(history,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6facdda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "train_test_accuracy(cnn_model,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report\n",
    "report_nn(cnn_model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41211722",
   "metadata": {},
   "source": [
    "#### d) GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f75416",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = GRU_model(embedding_matrix,vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f1934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = gru_model.fit(X_train, np.asarray(Y_train), validation_data=(X_val, np.asarray(Y_val)), batch_size = batch_size, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f800d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy\n",
    "plot_accuracy(history,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "train_test_accuracy(gru_model,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report\n",
    "report_nn(gru_model,X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3e6f3",
   "metadata": {},
   "source": [
    "#### e) CNN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_gru_model =CNN_GRU_model(embedding_matrix,vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history=cnn_gru_model.fit(X_train, np.asarray(Y_train), validation_data=(X_val, np.asarray(Y_val)),epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b447a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy\n",
    "plot_accuracy(history,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "train_test_accuracy(cnn_gru_model,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65751a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report\n",
    "report_nn(cnn_gru_model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4da46",
   "metadata": {},
   "source": [
    "#### f) CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model = CNN_LSTM_model(embedding_matrix,vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history=cnn_lstm_model.fit(X_train, np.asarray(Y_train), validation_data=(X_val, np.asarray(Y_val)),epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy\n",
    "plot_accuracy(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0697dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "train_test_accuracy(cnn_lstm_model,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc249b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report\n",
    "report_nn(cnn_lstm_model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d2838",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ced08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_lstm_model0=cnn_lstm_model\n",
    "print('--Confusion Matrix for CNN+LSTM--')\n",
    "print_confusion(cnn_lstm_model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1d892",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29038356",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmbd_model = BID_LSTM_model(embedding_matrix,vocab_len,emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21856f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history=lstmbd_model.fit(X_train, np.asarray(Y_train), validation_data=(X_val, np.asarray(Y_val)),epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ab166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy\n",
    "plot_accuracy(history,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "train_test_accuracy(cnn_lstm_model,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report\n",
    "report_nn(cnn_lstm_model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ddb5e",
   "metadata": {},
   "source": [
    "## Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a1db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grubd_model = BID_GRU_model(embedding_matrix,vocab_len,emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history=grubd_model.fit(X_train, np.asarray(Y_train), validation_data=(X_val, np.asarray(Y_val)),epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f10aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy\n",
    "plot_accuracy(history,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "train_test_accuracy(grubd_model,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe095440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Classification report\n",
    "report_nn(grubd_model,X_test,Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
